[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "GeonWook",
    "section": "",
    "text": "Order By\nDefault\n\n          Title\n        \n\n          Date - Oldest\n        \n\n          Date - Newest\n        \n\n\n\n\n\n \n\n\n\n\nE-commerce 데이터 분석 (2)\n\n\n\n\n\nData Mart 구성 및 Feature Engineering\n\n\n\n\n\n\n2023-12-08\n\n\n12 min\n\n\n\n\n\n\n \n\n\n\n\nE-commerce 데이터 분석 (1)\n\n\n\n\n\nUCI Machine Learning Repository: Online Retail Dataset\n\n\n\n\n\n\n2023-12-03\n\n\n8 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "1. GW_Blog",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n{r} 1 + 1"
  },
  {
    "objectID": "index.html#latest-posts",
    "href": "index.html#latest-posts",
    "title": "GeonWook",
    "section": "Latest Posts\n",
    "text": "Latest Posts"
  },
  {
    "objectID": "blog/2023-12-03-ecommerce-data-analysis/index.html",
    "href": "blog/2023-12-03-ecommerce-data-analysis/index.html",
    "title": "E-commerce 데이터 분석 (1)",
    "section": "",
    "text": "본 포스팅은 패스트캠퍼스 50개 프로젝트로 완벽하게 끝내는 머신러닝 시그니쳐의 강의내용을 바탕으로 참고하여 작성하였습니다."
  },
  {
    "objectID": "blog/2023-12-03-ecommerce-data-analysis/index.html#sec-1",
    "href": "blog/2023-12-03-ecommerce-data-analysis/index.html#sec-1",
    "title": "E-commerce 데이터 분석 (1)",
    "section": "01. 문제해결 프로세스 기획",
    "text": "01. 문제해결 프로세스 기획\n패스트캠퍼스 강의 중 박지환 강사님께서 준비 해주신 이커머스 데이터 분석 예제를 참고하여 진행하였습니다. 주어진 문제의 시나리오는 아래와 같습니다.\n\n\n\n\n\n\n시나리오\n\n\n\nA사는 이커머스 플랫폼을 운영하고 있다. 해당 이커머스 플랫폼 서비스가 성장하기 위해서는 3가지 조건이 필요하다.\n① 신규고객 유입 ② 유입 고객 재구매 ③ 재구매고객 충성 고객화\nA사는 현재 ’① 신규교객 유입 활동’은 마케팅 비용을 투자하여 적극 수행하고 있으나, ’② 유입 고객 재구매’이 미미한 상황이다. 충성고객의 Spending Power는 전체 매출에서 많은 비중을 차지하고 있으므로, 충성고객의 성장이 이어지지 않으면 플랫폼의 성장도 멈추게 되버린다.\n따라서 ’② 유입 고객 재구매’를 촉진시킬 방법을 고민 중에 있는 상황이다.\n\n\n이 시나리오를 기반으로 문제해결 프로세스 Figure 1 에 따라 7가지 단계를 아래와 같이 정의할 수 있습니다.\n\n\n\n\n\n\n문제해결 프로세스\n\n\n\nStep 1. 문제정의\n\n문제현상: 신규 고객 유입 후 재구매로 이어지는 고객의 감소\n예상 피해: (재구매고객 → 충성고객)의 부진으로 매출 성장 정체 및 신규고객 유입을 위한 마케팅 비용 증가로 인한 영업이익 감소\nStep 2. 기대효과\n\n재구매 고객 증가 → 충성 고객 증가 → 매출 성장 → 영업 이익 증가\n선순환 체계 구축으로 인한 서비스 성장\nStep 3. 해결방안\n\n① EDA 및 일회성 데이터 분석을 통해 재구매 고객의 특성을 분석하고 이를 토대로 마케팅 기획\n② 재구매 가능성이 높은 고객을 예측하는 모델링 후 이를 활용한 타겟 마케팅 진행\nStep 4. 우선순위\n\n①번을 빠르게 수행 후 파일럿 테스트 진행 및 성과 측정\n①번의 효과가 좋지 않다면, ②번 진행 후 파일럿 테스트 재실행\nStep 5. 데이터 분석\n\n결정된 우선순위에 따라서 데이터 분석 및 모델링 진행\nStep 6. 성과측정\n\n최종 마케팅 후 성능을 평가하기 위한 지표 수립\n분석 및 모델링을 통해 추출한 타겟 고객군과 대조군을 설정하여 A/B 테스트 수행\nA/B 테스트 결과 마케팅(재구매) 반응률 비교를 통해 통계적으로 유의미한지 검증: t-test\n\n유의미한 결과를 얻을 때까지 파일럿 테스트를 수정하며 진행\nStep 7. 모델 운영\n\n파일럿 테스트 후 결과가 유의미하다면 정규 마케팅으로 운영하기 위한 작업 준비\n모델을 실행을 위한 주기와 추출 타겟 고객군의 범위 결정\n정해진 주기에 따라 타겟 고객군의 추출을 자동화\n이를 마케팅 시스템과 연계하여 타겟 마케팅을 주기적으로 운영 및 평가\n\n\n\n현재 Step 1~4의 단계가 완료되어 데이터 분석을 수행해야 하는 단계라고 간주하겠습니다."
  },
  {
    "objectID": "blog/2023-12-03-ecommerce-data-analysis/index.html#data-readiness-check-sampling",
    "href": "blog/2023-12-03-ecommerce-data-analysis/index.html#data-readiness-check-sampling",
    "title": "E-commerce 데이터 분석 (1)",
    "section": "02. Data Readiness Check & Sampling",
    "text": "02. Data Readiness Check & Sampling\n사용하고자 하는 데이터는 UCI Machine Learning Repository에서 제공하는 Online Retail 데이터를 사용해 진행했습니다.\n이 Online Retail 데이터를 A사에서 수집한 고객 거래 이력 데이터로 간주하고 해당 데이터에 대한 기본적인 전처리를 수행하겠습니다.\n02-01. Data Info Check\n주어진 데이터는 e-commerce 온라인 구매 데이터로 데이터 명세표는 아래와 같습니다.\n\n\n\n\n\n\n\n\n\n\n\n\nInvoiceNo\nStockCode\nDescription\nQuantity\nInvoiceDate\nUnitPrice\nCustomerID\nCountry\n\n\n\n송장번호\n재고코드\n상세설명\n수량\n송장날짜\n개당가격\n고객ID\n국가\n\n\nCategorical\nCategorical\nCategorical\nInteger\nDate\nContinuous\nCategorical\nCategorical\n\n\n\n이제 수집된 데이터의 기본적인 정보를 확인해 보겠습니다.\n\nlibrary(tidyverse)\ndf &lt;- read_csv(\"Online Retail.csv\", col_types = \"ccciTdcc\")\ndf %&gt;% head()\n\n# A tibble: 6 × 8\n  InvoiceNo StockCode Description                           Quantity InvoiceDate         UnitPrice CustomerID Country       \n  &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;                                    &lt;int&gt; &lt;dttm&gt;                  &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;         \n1 489434    85048     \"15CM CHRISTMAS GLASS BALL 20 LIGHTS\"       12 2009-12-01 07:45:00      6.95 13085      United Kingdom\n2 489434    79323P    \"PINK CHERRY LIGHTS\"                        12 2009-12-01 07:45:00      6.75 13085      United Kingdom\n3 489434    79323W    \"WHITE CHERRY LIGHTS\"                       12 2009-12-01 07:45:00      6.75 13085      United Kingdom\n4 489434    22041     \"RECORD FRAME 7\\\" SINGLE SIZE\"              48 2009-12-01 07:45:00      2.1  13085      United Kingdom\n5 489434    21232     \"STRAWBERRY CERAMIC TRINKET BOX\"            24 2009-12-01 07:45:00      1.25 13085      United Kingdom\n6 489434    22064     \"PINK DOUGHNUT TRINKET POT\"                 24 2009-12-01 07:45:00      1.65 13085      United Kingdom\n\n\nData 크기 확인\n\n# 1,067,370 rows & 8 cols\ndf %&gt;% dim()\n\n[1] 1067370       8\n\n\nData type 확인\n\ndf %&gt;% glimpse()\n\nRows: 1,067,370\nColumns: 8\n$ InvoiceNo   &lt;chr&gt; \"489434\", \"489434\", \"489434\", \"489434\", \"489434\", \"489434\", \"489434\", \"489434\", \"489435\", \"489435\", \"489435\", \"489435\", \"489436\", \"489436\", \"489436\", \"489436\", \"489436\", \"489436\"…\n$ StockCode   &lt;chr&gt; \"85048\", \"79323P\", \"79323W\", \"22041\", \"21232\", \"22064\", \"21871\", \"21523\", \"22350\", \"22349\", \"22195\", \"22353\", \"48173C\", \"21755\", \"21754\", \"84879\", \"22119\", \"22142\", \"22296\", \"222…\n$ Description &lt;chr&gt; \"15CM CHRISTMAS GLASS BALL 20 LIGHTS\", \"PINK CHERRY LIGHTS\", \"WHITE CHERRY LIGHTS\", \"RECORD FRAME 7\\\" SINGLE SIZE\", \"STRAWBERRY CERAMIC TRINKET BOX\", \"PINK DOUGHNUT TRINKET POT\",…\n$ Quantity    &lt;int&gt; 12, 12, 12, 48, 24, 24, 24, 10, 12, 12, 24, 12, 10, 18, 3, 16, 3, 12, 12, 12, 16, 4, 2, 12, 12, 12, 3, 6, 8, 8, 24, 6, 6, 12, 2, 1, 2, 2, 2, 3, 12, 12, 6, 3, 12, 12, 12, 12, 6, 6…\n$ InvoiceDate &lt;dttm&gt; 2009-12-01 07:45:00, 2009-12-01 07:45:00, 2009-12-01 07:45:00, 2009-12-01 07:45:00, 2009-12-01 07:45:00, 2009-12-01 07:45:00, 2009-12-01 07:45:00, 2009-12-01 07:45:00, 2009-12-0…\n$ UnitPrice   &lt;dbl&gt; 6.95, 6.75, 6.75, 2.10, 1.25, 1.65, 1.25, 5.95, 2.55, 3.75, 1.65, 2.55, 5.95, 5.45, 5.95, 1.69, 6.95, 1.45, 1.65, 1.65, 3.39, 3.75, 8.50, 4.65, 2.10, 2.10, 5.95, 2.95, 1.25, 1.25…\n$ CustomerID  &lt;chr&gt; \"13085\", \"13085\", \"13085\", \"13085\", \"13085\", \"13085\", \"13085\", \"13085\", \"13085\", \"13085\", \"13085\", \"13085\", \"13078\", \"13078\", \"13078\", \"13078\", \"13078\", \"13078\", \"13078\", \"13078\"…\n$ Country     &lt;chr&gt; \"United Kingdom\", \"United Kingdom\", \"United Kingdom\", \"United Kingdom\", \"United Kingdom\", \"United Kingdom\", \"United Kingdom\", \"United Kingdom\", \"United Kingdom\", \"United Kingdom\"…\n\n\n변수별 결측치 확인\n\ndf %&gt;% is.na() %&gt;% colSums()\n\n  InvoiceNo   StockCode Description    Quantity InvoiceDate   UnitPrice  CustomerID     Country \n          0           0        4382           0           0           0      243007           0 \n\n\n고객 식별 정보인 CustomerID 같은 변수에 결측치가 있는 경우에는 추후에 해당 고객에 대한 정보를 알 수 없으므로 사전에 정보를 알 수 없는 불분명한 데이터로 취급하여 삭제하는 것이 좋습니다.\n\ndf &lt;- df %&gt;% drop_na(\"CustomerID\")\ndf %&gt;% is.na() %&gt;% colSums()\n\n  InvoiceNo   StockCode Description    Quantity InvoiceDate   UnitPrice  CustomerID     Country \n          0           0           0           0           0           0           0           0 \n\n\nOutlier 확인\n\ndf %&gt;% \n  select_if(is.numeric) %&gt;% \n  summary()\n\n    Quantity           UnitPrice       \n Min.   :-80995.00   Min.   :    0.00  \n 1st Qu.:     2.00   1st Qu.:    1.25  \n Median :     5.00   Median :    1.95  \n Mean   :    12.41   Mean   :    3.68  \n 3rd Qu.:    12.00   3rd Qu.:    3.75  \n Max.   : 80995.00   Max.   :38970.00  \n\n\n구매 수량을 나타내는 Quantity는 음수 값을 가질 수 없으나, 반품 물품에 대한 값이 있으므로 확인 후 처리해야 합니다. 여기서는 Quantity가 음수인 값은 이상치(Outlier)라고 간주하고 삭제하겠습니다.\n\n# Quantity가 음수인 데이터 확인 \n# 반품 및 회수 물량일 수도 있음\ndf %&gt;% filter(Quantity &lt; 0)\n\n# A tibble: 18,744 × 8\n   InvoiceNo StockCode Description                       Quantity InvoiceDate         UnitPrice CustomerID Country       \n   &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;                                &lt;int&gt; &lt;dttm&gt;                  &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;         \n 1 C489449   22087     PAPER BUNTING WHITE LACE               -12 2009-12-01 10:33:00      2.95 16321      Australia     \n 2 C489449   85206A    CREAM FELT EASTER EGG BASKET            -6 2009-12-01 10:33:00      1.65 16321      Australia     \n 3 C489449   21895     POTTING SHED SOW 'N' GROW SET           -4 2009-12-01 10:33:00      4.25 16321      Australia     \n 4 C489449   21896     POTTING SHED TWINE                      -6 2009-12-01 10:33:00      2.1  16321      Australia     \n 5 C489449   22083     PAPER CHAIN KIT RETRO SPOT             -12 2009-12-01 10:33:00      2.95 16321      Australia     \n 6 C489449   21871     SAVE THE PLANET MUG                    -12 2009-12-01 10:33:00      1.25 16321      Australia     \n 7 C489449   84946     ANTIQUE SILVER TEA GLASS ETCHED        -12 2009-12-01 10:33:00      1.25 16321      Australia     \n 8 C489449   84970S    HANGING HEART ZINC T-LIGHT HOLDER      -24 2009-12-01 10:33:00      0.85 16321      Australia     \n 9 C489449   22090     PAPER BUNTING RETRO SPOTS              -12 2009-12-01 10:33:00      2.95 16321      Australia     \n10 C489459   90200A    PURPLE SWEETHEART BRACELET              -3 2009-12-01 10:44:00      4.25 17592      United Kingdom\n# ℹ 18,734 more rows\n\n\n\n# Quantity 음수값 제거\ndf &lt;- df %&gt;% filter(Quantity &gt; 0)"
  },
  {
    "objectID": "blog/2023-12-03-ecommerce-data-analysis/index.html#data-info-check",
    "href": "blog/2023-12-03-ecommerce-data-analysis/index.html#data-info-check",
    "title": "E-commerce 데이터 분석 (1)",
    "section": "02-01. Data Info Check",
    "text": "02-01. Data Info Check\n주어진 데이터는 e-commerce 온라인 구매 데이터로 데이터 명세표는 아래와 같습니다.\n\n\n\n\n\n\n\n\n\n\n\n\nInvoiceNo\nStockCode\nDescription\nQuantity\nInvoiceDate\nUnitPrice\nCustomerID\nCountry\n\n\n\n송장번호\n재고코드\n상세설명\n수량\n송장날짜\n개당가격\n고객ID\n국가\n\n\nCategorical\nCategorical\nCategorical\nInteger\nDate\nContinuous\nCategorical\nCategorical\n\n\n\n이제 수집된 데이터의 기본적인 정보를 확인해 보겠습니다.\n\nlibrary(tidyverse)\ndf &lt;- read_csv(\"Online Retail.csv\", col_types = \"ccciTdcc\")\ndf %&gt;% head()\n\n# A tibble: 6 × 8\n  InvoiceNo StockCode Description                           Quantity InvoiceDate         UnitPrice CustomerID Country       \n  &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;                                    &lt;int&gt; &lt;dttm&gt;                  &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;         \n1 489434    85048     \"15CM CHRISTMAS GLASS BALL 20 LIGHTS\"       12 2009-12-01 07:45:00      6.95 13085      United Kingdom\n2 489434    79323P    \"PINK CHERRY LIGHTS\"                        12 2009-12-01 07:45:00      6.75 13085      United Kingdom\n3 489434    79323W    \"WHITE CHERRY LIGHTS\"                       12 2009-12-01 07:45:00      6.75 13085      United Kingdom\n4 489434    22041     \"RECORD FRAME 7\\\" SINGLE SIZE\"              48 2009-12-01 07:45:00      2.1  13085      United Kingdom\n5 489434    21232     \"STRAWBERRY CERAMIC TRINKET BOX\"            24 2009-12-01 07:45:00      1.25 13085      United Kingdom\n6 489434    22064     \"PINK DOUGHNUT TRINKET POT\"                 24 2009-12-01 07:45:00      1.65 13085      United Kingdom\n\n\nData 크기 확인\n\n# 1,067,370 rows & 8 cols\ndf %&gt;% dim()\n\n[1] 1067370       8\n\n\nData type 확인\n\ndf %&gt;% glimpse()\n\nRows: 1,067,370\nColumns: 8\n$ InvoiceNo   &lt;chr&gt; \"489434\", \"489434\", \"489434\", \"489434\", \"489434\", \"489434\", \"489434\", \"489434\", \"489435\", \"489435\", \"489435\", \"489435\", \"489436\", \"489436\", \"489436\", \"489436\", \"489436\", \"489436\"…\n$ StockCode   &lt;chr&gt; \"85048\", \"79323P\", \"79323W\", \"22041\", \"21232\", \"22064\", \"21871\", \"21523\", \"22350\", \"22349\", \"22195\", \"22353\", \"48173C\", \"21755\", \"21754\", \"84879\", \"22119\", \"22142\", \"22296\", \"222…\n$ Description &lt;chr&gt; \"15CM CHRISTMAS GLASS BALL 20 LIGHTS\", \"PINK CHERRY LIGHTS\", \"WHITE CHERRY LIGHTS\", \"RECORD FRAME 7\\\" SINGLE SIZE\", \"STRAWBERRY CERAMIC TRINKET BOX\", \"PINK DOUGHNUT TRINKET POT\",…\n$ Quantity    &lt;int&gt; 12, 12, 12, 48, 24, 24, 24, 10, 12, 12, 24, 12, 10, 18, 3, 16, 3, 12, 12, 12, 16, 4, 2, 12, 12, 12, 3, 6, 8, 8, 24, 6, 6, 12, 2, 1, 2, 2, 2, 3, 12, 12, 6, 3, 12, 12, 12, 12, 6, 6…\n$ InvoiceDate &lt;dttm&gt; 2009-12-01 07:45:00, 2009-12-01 07:45:00, 2009-12-01 07:45:00, 2009-12-01 07:45:00, 2009-12-01 07:45:00, 2009-12-01 07:45:00, 2009-12-01 07:45:00, 2009-12-01 07:45:00, 2009-12-0…\n$ UnitPrice   &lt;dbl&gt; 6.95, 6.75, 6.75, 2.10, 1.25, 1.65, 1.25, 5.95, 2.55, 3.75, 1.65, 2.55, 5.95, 5.45, 5.95, 1.69, 6.95, 1.45, 1.65, 1.65, 3.39, 3.75, 8.50, 4.65, 2.10, 2.10, 5.95, 2.95, 1.25, 1.25…\n$ CustomerID  &lt;chr&gt; \"13085\", \"13085\", \"13085\", \"13085\", \"13085\", \"13085\", \"13085\", \"13085\", \"13085\", \"13085\", \"13085\", \"13085\", \"13078\", \"13078\", \"13078\", \"13078\", \"13078\", \"13078\", \"13078\", \"13078\"…\n$ Country     &lt;chr&gt; \"United Kingdom\", \"United Kingdom\", \"United Kingdom\", \"United Kingdom\", \"United Kingdom\", \"United Kingdom\", \"United Kingdom\", \"United Kingdom\", \"United Kingdom\", \"United Kingdom\"…\n\n\n변수별 결측치 확인\n\ndf %&gt;% is.na() %&gt;% colSums()\n\n  InvoiceNo   StockCode Description    Quantity InvoiceDate   UnitPrice  CustomerID     Country \n          0           0        4382           0           0           0      243007           0 \n\n\n고객 식별 정보인 CustomerID 같은 변수에 결측치가 있는 경우에는 추후에 해당 고객에 대한 정보를 알 수 없으므로 사전에 정보를 알 수 없는 불분명한 데이터로 취급하여 삭제하는 것이 좋습니다.\n\ndf &lt;- df %&gt;% drop_na(\"CustomerID\")\ndf %&gt;% is.na() %&gt;% colSums()\n\n  InvoiceNo   StockCode Description    Quantity InvoiceDate   UnitPrice  CustomerID     Country \n          0           0           0           0           0           0           0           0 \n\n\nOutlier 확인\n\ndf %&gt;% \n  select_if(is.numeric) %&gt;% \n  summary()\n\n    Quantity           UnitPrice       \n Min.   :-80995.00   Min.   :    0.00  \n 1st Qu.:     2.00   1st Qu.:    1.25  \n Median :     5.00   Median :    1.95  \n Mean   :    12.41   Mean   :    3.68  \n 3rd Qu.:    12.00   3rd Qu.:    3.75  \n Max.   : 80995.00   Max.   :38970.00  \n\n\n구매 수량을 나타내는 Quantity는 음수 값을 가질 수 없으나, 반품 물품에 대한 값이 있으므로 확인 후 처리해야 합니다. 여기서는 Quantity가 음수인 값은 이상치(Outlier)라고 간주하고 삭제하겠습니다.\n\n# Quantity가 음수인 데이터 확인 \n# 반품 및 회수 물량일 수도 있음\ndf %&gt;% filter(Quantity &lt; 0)\n\n# A tibble: 18,744 × 8\n   InvoiceNo StockCode Description                       Quantity InvoiceDate         UnitPrice CustomerID Country       \n   &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;                                &lt;int&gt; &lt;dttm&gt;                  &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;         \n 1 C489449   22087     PAPER BUNTING WHITE LACE               -12 2009-12-01 10:33:00      2.95 16321      Australia     \n 2 C489449   85206A    CREAM FELT EASTER EGG BASKET            -6 2009-12-01 10:33:00      1.65 16321      Australia     \n 3 C489449   21895     POTTING SHED SOW 'N' GROW SET           -4 2009-12-01 10:33:00      4.25 16321      Australia     \n 4 C489449   21896     POTTING SHED TWINE                      -6 2009-12-01 10:33:00      2.1  16321      Australia     \n 5 C489449   22083     PAPER CHAIN KIT RETRO SPOT             -12 2009-12-01 10:33:00      2.95 16321      Australia     \n 6 C489449   21871     SAVE THE PLANET MUG                    -12 2009-12-01 10:33:00      1.25 16321      Australia     \n 7 C489449   84946     ANTIQUE SILVER TEA GLASS ETCHED        -12 2009-12-01 10:33:00      1.25 16321      Australia     \n 8 C489449   84970S    HANGING HEART ZINC T-LIGHT HOLDER      -24 2009-12-01 10:33:00      0.85 16321      Australia     \n 9 C489449   22090     PAPER BUNTING RETRO SPOTS              -12 2009-12-01 10:33:00      2.95 16321      Australia     \n10 C489459   90200A    PURPLE SWEETHEART BRACELET              -3 2009-12-01 10:44:00      4.25 17592      United Kingdom\n# ℹ 18,734 more rows\n\n\n\n# Quantity 음수값 제거\ndf &lt;- df %&gt;% filter(Quantity &gt; 0)\n\n중복 데이터 확인\n\ndf %&gt;% \n  mutate(duplicated = duplicated(.)) %&gt;% \n  count(duplicated)\n\n# A tibble: 2 × 2\n  duplicated      n\n  &lt;lgl&gt;       &lt;int&gt;\n1 FALSE      779494\n2 TRUE        26125\n\n# 중복 데이터 확인\ndf %&gt;% \n  filter(duplicated(df) | duplicated(df, fromLast = TRUE))\n\n# A tibble: 50,838 × 8\n   InvoiceNo StockCode Description                       Quantity InvoiceDate         UnitPrice CustomerID Country       \n   &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;                                &lt;int&gt; &lt;dttm&gt;                  &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;         \n 1 489517    21913     VINTAGE SEASIDE JIGSAW PUZZLES           1 2009-12-01 11:34:00      3.75 16329      United Kingdom\n 2 489517    21912     VINTAGE SNAKES & LADDERS                 1 2009-12-01 11:34:00      3.75 16329      United Kingdom\n 3 489517    21821     GLITTER STAR GARLAND WITH BELLS          1 2009-12-01 11:34:00      3.75 16329      United Kingdom\n 4 489517    22319     HAIRCLIPS FORTIES FABRIC ASSORTED       12 2009-12-01 11:34:00      0.65 16329      United Kingdom\n 5 489517    22130     PARTY CONE CHRISTMAS DECORATION          6 2009-12-01 11:34:00      0.85 16329      United Kingdom\n 6 489517    21912     VINTAGE SNAKES & LADDERS                 1 2009-12-01 11:34:00      3.75 16329      United Kingdom\n 7 489517    21491     SET OF THREE VINTAGE GIFT WRAPS          1 2009-12-01 11:34:00      1.95 16329      United Kingdom\n 8 489517    22130     PARTY CONE CHRISTMAS DECORATION          6 2009-12-01 11:34:00      0.85 16329      United Kingdom\n 9 489517    22319     HAIRCLIPS FORTIES FABRIC ASSORTED       12 2009-12-01 11:34:00      0.65 16329      United Kingdom\n10 489517    21913     VINTAGE SEASIDE JIGSAW PUZZLES           1 2009-12-01 11:34:00      3.75 16329      United Kingdom\n# ℹ 50,828 more rows\n\n\n26,125개의 데이터가 중복되었음을 확인할 수 있습니다. 데이터의 정확성과 품질을 위해 중복 데이터를 삭제하겠습니다.\n\n# 중복 삭제 후 재확인\ndf &lt;- df %&gt;% distinct()\ndf %&gt;% \n  mutate(duplicated = duplicated(.)) %&gt;% \n  count()\n\n# A tibble: 1 × 1\n       n\n   &lt;int&gt;\n1 779494\n\n\n이렇게 데이터에 대한 기본적인 정보와 전처리를 마치고 데이터의 형태를 보면 아래와 같이 기존 1,067,370개의 행에서 779,494개의 행으로 줄었습니다.\n\n# 1,067,370 → 779,494\ndf %&gt;% dim()\n\n[1] 779494      8"
  },
  {
    "objectID": "blog/2023-12-03-ecommerce-data-analysis/index.html#data-readiness-check",
    "href": "blog/2023-12-03-ecommerce-data-analysis/index.html#data-readiness-check",
    "title": "E-commerce 데이터 분석 (1)",
    "section": "02-03. Data Readiness Check",
    "text": "02-03. Data Readiness Check\n데이터에 대한 기본 정보를 확인했으니 이제 현재 가지고 있는 데이터로 앞서 기획한 문제해결 프로세스를 적용할 수 있는지 점검해야 합니다.\n(1) Target Label 생성\n당월 구매 고객이 다음 달에 재구매 시 해당 고객을 재구매 고객으로 정의하겠습니다. 예를들어, 2011년 01월에 구매한 고객이 2011년 02월에 구매를 한다면 해당 고객을 재구매 고객으로 정의하는 것입니다.\n\nlibrary(lubridate)\n\n# 기준년월 변수 생성: bsym - %Y-%m 형식\ndf &lt;- df %&gt;% mutate(bsym = format(InvoiceDate, \"%Y-%m\")) \n# 원본 데이터 저장: df_origin\ndf_origin &lt;- df\n# 데이터 적재 기간 확인: 2009-12-01 ~ 2011-12-09 (약 2년)\nmin(df$InvoiceDate); max(df$InvoiceDate)\n\n[1] \"2009-12-01 07:45:00 UTC\"\n\n\n[1] \"2011-12-09 12:50:00 UTC\"\n\n\n기본적인 전처리가 완료된 데이터를 df_origin으로 저장하여 보존하고, 이후 있을 전처리를 진행하겠습니다. Target label은 기준년월(bsym)과 고객(CustomerID)에만 영향을 받으므로 해당 두 변수를 고유하게 갖는 데이터로 재구성하겠습니다.\n\ndf &lt;- df %&gt;% distinct(bsym, CustomerID)\ndf %&gt;% dim()\n\n[1] 25598     2\n\ndf %&gt;% head()\n\n# A tibble: 6 × 2\n  bsym    CustomerID\n  &lt;chr&gt;   &lt;chr&gt;     \n1 2009-12 13085     \n2 2009-12 13078     \n3 2009-12 15362     \n4 2009-12 18102     \n5 2009-12 12682     \n6 2009-12 18087     \n\n\n2009년 12월부터 2011년 12월까지 연도 및 월(bsym)별로 당월 구매 고객이 내월 구매 고객일 경우 1을 갖는 target 변수(binary: 0,1)를 만들어 보겠습니다.\n\n# 주어진 bsym에 구매 고객이 내월 구매 고객일 경우 1, 그렇지 않으면 0 값을 갖는 \n# target 변수 생성: process_bsym()\nprocess_bsym &lt;- function(bsym_value, df) {\n  df_left &lt;- filter(df, bsym == bsym_value)\n  bsym_1m &lt;- ymd(paste0(bsym_value, \"-01\")) %m+% months(1) %&gt;% format(\"%Y-%m\")\n  df_right &lt;- df %&gt;% \n    filter(bsym == bsym_1m) %&gt;% \n    distinct(CustomerID) %&gt;% \n    mutate(target = 1)\n  \n  df_merge &lt;- left_join(df_left, df_right, by = \"CustomerID\") %&gt;% \n    mutate(target = ifelse(is.na(target), 0, target))\n  \n  return(df_merge)\n}\n\n# 모든 bsym 값에 대해 process_bsym 함수 적용\ndf_all &lt;- map_df(unique(df$bsym), ~process_bsym(.x, df))\ndf_all %&gt;% head()\n\n# A tibble: 6 × 3\n  bsym    CustomerID target\n  &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt;\n1 2009-12 13085           1\n2 2009-12 13078           1\n3 2009-12 15362           0\n4 2009-12 18102           1\n5 2009-12 12682           1\n6 2009-12 18087           1\n\n\n(2) Target Ratio 확인\n연도 및 월별로 재구매 고객 여부 변수 target의 비율을 확인해 보겠습니다.\n\n# 기준년월 기준 Target ratio 확인\noptions(pillar.sigfig = 6)\ndf_target &lt;- df_all %&gt;% \n  group_by(bsym) %&gt;% \n  reframe(total_y = sum(target),\n          count_y = n(),\n          ratio = total_y/count_y) \ndf_target %&gt;% \n  print(n =  nrow(.))\n\n# A tibble: 25 × 4\n   bsym    total_y count_y    ratio\n   &lt;chr&gt;     &lt;dbl&gt;   &lt;int&gt;    &lt;dbl&gt;\n 1 2009-12     337     955 0.352880\n 2 2010-01     262     720 0.363889\n 3 2010-02     314     774 0.405685\n 4 2010-03     378    1057 0.357616\n 5 2010-04     345     942 0.366242\n 6 2010-05     368     966 0.380952\n 7 2010-06     392    1041 0.376561\n 8 2010-07     351     928 0.378233\n 9 2010-08     365     911 0.400659\n10 2010-09     463    1145 0.404367\n11 2010-10     657    1497 0.438878\n12 2010-11     524    1607 0.326073\n13 2010-12     324     885 0.366102\n14 2011-01     262     741 0.353576\n15 2011-02     290     758 0.382586\n16 2011-03     304     974 0.312115\n17 2011-04     368     856 0.429907\n18 2011-05     410    1056 0.388258\n19 2011-06     365     991 0.368315\n20 2011-07     388     949 0.408851\n21 2011-08     425     935 0.454545\n22 2011-09     489    1266 0.386256\n23 2011-10     622    1364 0.456012\n24 2011-11     371    1665 0.222823\n25 2011-12       0     615 0       \n\n\n현재 주어진 데이터는 2011년 12월이 마지막이기 때문에 내월이 없는 2011년 12월은 target 변수가 1 값을 가질 수 없습니다. 따라서 분석 대상에서 제외시키겠습니다.\n\ndf_target &lt;- df_target %&gt;% filter(bsym != '2011-12')\ndf_all &lt;- df_all %&gt;% filter(bsym != '2011-12')\ndf_target %&gt;% \n  print(n = nrow(.))\n\n# A tibble: 24 × 4\n   bsym    total_y count_y    ratio\n   &lt;chr&gt;     &lt;dbl&gt;   &lt;int&gt;    &lt;dbl&gt;\n 1 2009-12     337     955 0.352880\n 2 2010-01     262     720 0.363889\n 3 2010-02     314     774 0.405685\n 4 2010-03     378    1057 0.357616\n 5 2010-04     345     942 0.366242\n 6 2010-05     368     966 0.380952\n 7 2010-06     392    1041 0.376561\n 8 2010-07     351     928 0.378233\n 9 2010-08     365     911 0.400659\n10 2010-09     463    1145 0.404367\n11 2010-10     657    1497 0.438878\n12 2010-11     524    1607 0.326073\n13 2010-12     324     885 0.366102\n14 2011-01     262     741 0.353576\n15 2011-02     290     758 0.382586\n16 2011-03     304     974 0.312115\n17 2011-04     368     856 0.429907\n18 2011-05     410    1056 0.388258\n19 2011-06     365     991 0.368315\n20 2011-07     388     949 0.408851\n21 2011-08     425     935 0.454545\n22 2011-09     489    1266 0.386256\n23 2011-10     622    1364 0.456012\n24 2011-11     371    1665 0.222823\n\ndf_target$ratio %&gt;% summary()\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.2228  0.3623  0.3796  0.3784  0.4047  0.4560 \n\n\n연도 및 월(bsym)별 target의 비율은 22.3% ~ 45.6%의 값을 가지며, 대부분의 월에서 36% ~ 40% 정도의 비율을 갖습니다. 전체 데이터의 target의 비율을 보면 아래와 같이 약 37.5% 정도의 내달 재구매 비율을 갖습니다. 이 정도의 class imbalance에 대해서는 over-sampling을 하지 않고 분석을 진행해도 괜찮을 것 같습니다.\n\ndf_all$target %&gt;% mean()\n\n[1] 0.3752151\n\n\n\n# 마지막 행에 합계 추가\ndf_target &lt;- df_target %&gt;% \n  bind_rows(\n    tibble(bsym = \"total\",\n           total_y = sum(.$total_y),\n           count_y = sum(.$count_y),\n           ratio = 1)  \n)"
  },
  {
    "objectID": "blog/2023-12-03-ecommerce-data-analysis/index.html#under-sampling-stratified-sampling",
    "href": "blog/2023-12-03-ecommerce-data-analysis/index.html#under-sampling-stratified-sampling",
    "title": "E-commerce 데이터 분석 (1)",
    "section": "03-01. Under-sampling: Stratified sampling",
    "text": "03-01. Under-sampling: Stratified sampling\n관심 변수(target)에 대해 함께 살펴보고 싶은 변수가 있을 시(여기선 bsym), 해당 변수를 층화 변수로 두어 층화추출법(stratified sampling)을 실시할 수 있습니다. 샘플링 기법 중 하나인 층화추출법은 모집단을 층화 변수로 나눈 후 각 층/소집단 별로 독립적으로 표본을 뽑는 샘플링 방법입니다. 이로 인해 모집단 데이터에서 층화 변수와 관심 변수가 갖는 관계를 표본 데이터에서도 유지할 수 있다는 장점이 있습니다. 즉, 표본의 대표성을 확보할 수 있는 방법입니다.\n현재 df_all 데이터를 모집단 데이터로 간주하고, 표본 데이터를 뽑고자 합니다. 이때 층화추출법을 사용하기 위해선 우선적으로 각 층(bsym)별 표본 크기가 주어져야 합니다. 이 과정을 표본 배분(sample allocation)이라고 합니다. 여기에서는 모집단 df_all의 bsym별 층 크기를 유지하기 위해 비례 배분(proportional allocation)을 사용하겠습니다. 비례 배분에 따른 표본 데이터의 표본 층 크기는 아래와 같습니다.\n\\[\nn_h = n\\times\\frac{N_h}{\\sum_{l=1}^HN_l}\n\\]\ndf_all 데이터의 30%만 샘플링한다고 하겠습니다. 그럴 경우 필요한 층(bsym)별 표본 크기(nh)는 아래와 같습니다.\n\n# 전체 데이터의 30%: 7495개\nround(nrow(df_all) * 0.3)\n\n[1] 7495\n\n# 모집단 층 크기: Nh, 표본 층 크기: nh\ndf_target &lt;- df_target %&gt;% \n  mutate(N = nrow(df_all)) %&gt;% \n  mutate(n = round(nrow(df_all) * 0.3)) %&gt;% \n  relocate(count_y, .after = n) %&gt;% \n  rename(Nh = count_y) %&gt;% \n  mutate(nh = round(n*Nh/N)) %&gt;% \n  group_by(bsym)\n\ndf_target %&gt;% print(n=25)\n\n# A tibble: 25 × 7\n# Groups:   bsym [25]\n   bsym    total_y    ratio     N     n    Nh    nh\n   &lt;chr&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;\n 1 2009-12     337 0.352880 24983  7495   955   287\n 2 2010-01     262 0.363889 24983  7495   720   216\n 3 2010-02     314 0.405685 24983  7495   774   232\n 4 2010-03     378 0.357616 24983  7495  1057   317\n 5 2010-04     345 0.366242 24983  7495   942   283\n 6 2010-05     368 0.380952 24983  7495   966   290\n 7 2010-06     392 0.376561 24983  7495  1041   312\n 8 2010-07     351 0.378233 24983  7495   928   278\n 9 2010-08     365 0.400659 24983  7495   911   273\n10 2010-09     463 0.404367 24983  7495  1145   344\n11 2010-10     657 0.438878 24983  7495  1497   449\n12 2010-11     524 0.326073 24983  7495  1607   482\n13 2010-12     324 0.366102 24983  7495   885   266\n14 2011-01     262 0.353576 24983  7495   741   222\n15 2011-02     290 0.382586 24983  7495   758   227\n16 2011-03     304 0.312115 24983  7495   974   292\n17 2011-04     368 0.429907 24983  7495   856   257\n18 2011-05     410 0.388258 24983  7495  1056   317\n19 2011-06     365 0.368315 24983  7495   991   297\n20 2011-07     388 0.408851 24983  7495   949   285\n21 2011-08     425 0.454545 24983  7495   935   281\n22 2011-09     489 0.386256 24983  7495  1266   380\n23 2011-10     622 0.456012 24983  7495  1364   409\n24 2011-11     371 0.222823 24983  7495  1665   500\n25 total      9374 1        24983  7495 24983  7495\n\n# 반올림 때문에 표본 층 크기가 n=7495 보다 1개 더 크므로 \n# 2011년 11월 표본 크기를 한 개 줄여주자\ndf_target$nh[df_target$bsym=='2011-11'] &lt;- df_target$nh[df_target$bsym=='2011-11'] - 1\n\n이제 주어진 표본배분안에 따라 bsym별 30% 정도의 표본을 뽑아보겠습니다.\n\nset.seed(123)\nord &lt;- unique(df_target$bsym)\nunits &lt;- sampling::strata(df_all, stratanames = \"bsym\", size = df_target$nh[1:24], method=\"srswor\")\n\ndf_all_sample &lt;- df_all %&gt;% \n  slice(units$ID_unit)\ndf_all_sample %&gt;% dim()\n\n[1] 7495    3\n\n\n이제 층화추출 후 표본 데이터 df_all_sample이 기존 모집단 데이터인 df_all의 bsym 별 target의 비율을 유지하는지 확인해 보겠습니다.\n\nCodelibrary(gt)\nlibrary(gtExtras)\ndf_tmp &lt;- df_all_sample %&gt;% \n  group_by(bsym) %&gt;% \n  reframe(sum_y = sum(target),\n          nh = n())\n   \ndf_tmp2 &lt;- df_tmp %&gt;% \n  mutate(target_ratio = sum_y/nh,\n         target_ratio_pop = df_target$ratio[1:24]) %&gt;% \n  pivot_longer(cols=c(target_ratio, target_ratio_pop), names_to = 'name') %&gt;% \n  group_by(bsym) %&gt;% \n  reframe(target_ratio = list(value))\ndf_tmp %&gt;% left_join(df_tmp2, by='bsym') %&gt;% \n  gt() %&gt;% \n  gt_theme_nytimes() %&gt;% \n  tab_header(title = \"bsym별 target 비율\") %&gt;% \n  gt_plt_bar_stack(column = target_ratio, labels = c(\"target_ratio\", \"target_ratio_pop\"), palette = c(\"skyblue\", \"hotpink\"))\n\n\n\n\n\n\n\nbsym별 target 비율\n    \n\nbsym\n      sum_y\n      nh\n      \ntarget_ratio||target_ratio_pop\n\n    \n\n\n\n2009-12\n111\n287\n\n0.3870.353\n\n\n\n2010-01\n79\n216\n\n0.36570.3639\n\n\n\n2010-02\n78\n232\n\n0.3360.406\n\n\n\n2010-03\n116\n317\n\n0.36590.3576\n\n\n\n2010-04\n103\n283\n\n0.36400.3662\n\n\n\n2010-05\n107\n290\n\n0.3690.381\n\n\n\n2010-06\n131\n312\n\n0.4200.377\n\n\n\n2010-07\n102\n278\n\n0.3670.378\n\n\n\n2010-08\n108\n273\n\n0.39560.4007\n\n\n\n2010-09\n141\n344\n\n0.40990.4044\n\n\n\n2010-10\n189\n449\n\n0.4210.439\n\n\n\n2010-11\n166\n482\n\n0.3440.326\n\n\n\n2010-12\n120\n266\n\n0.4510.366\n\n\n\n2011-01\n77\n222\n\n0.34680.3536\n\n\n\n2011-02\n84\n227\n\n0.3700.383\n\n\n\n2011-03\n91\n292\n\n0.311640.31211\n\n\n\n2011-04\n123\n257\n\n0.4790.430\n\n\n\n2011-05\n121\n317\n\n0.38170.3883\n\n\n\n2011-06\n110\n297\n\n0.37040.3683\n\n\n\n2011-07\n114\n285\n\n0.40000.4089\n\n\n\n2011-08\n136\n281\n\n0.4840.455\n\n\n\n2011-09\n138\n380\n\n0.3630.386\n\n\n\n2011-10\n196\n409\n\n0.4790.456\n\n\n\n2011-11\n117\n499\n\n0.2340.223\n\n\n\n\n\n\n\n\n# 표본 전체 target ratio: 38.1%\n# 모집단 전체 target ratio: 37.5%\ndf_all_sample$target %&gt;% mean()\n\n[1] 0.3813209\n\ndf_all$target %&gt;% mean()\n\n[1] 0.3752151\n\n\n이렇게 구성한 df_all_sample 데이터를 가지고 Data Mart를 만드는 과정을 다음 포스팅에서 다루겠습니다."
  },
  {
    "objectID": "blog/2023-12-03-ecommerce-data-analysis/index.html#data-sampling",
    "href": "blog/2023-12-03-ecommerce-data-analysis/index.html#data-sampling",
    "title": "E-commerce 데이터 분석 (1)",
    "section": "02-03. Data Sampling",
    "text": "02-03. Data Sampling\n\n\nFigure 2: 층화추출법\n\n현재 df_all에는 약 25,000건의 데이터가 있지만, 만일 데이터가 1억건 정도로 매우 큰 경우 모델링을 하기엔 부담스러울 수 있습니다. 이럴 경우를 대비해 해당 데이터를 모집단으로 간주하고 샘플링을 통해 데이터를 줄일 수 있습니다.\nUnder-sampling: Stratified sampling\n관심 변수(target)에 대해 함께 살펴보고 싶은 변수가 있을 시(여기선 bsym), 해당 변수를 층화 변수로 두어 층화추출법(stratified sampling)을 실시할 수 있습니다. 샘플링 기법 중 하나인 층화추출법은 모집단을 층화 변수로 나눈 후 각 층/소집단 별로 독립적으로 표본을 뽑는 샘플링 방법입니다. 이로 인해 모집단 데이터에서 층화 변수와 관심 변수가 갖는 관계를 표본 데이터에서도 유지할 수 있다는 장점이 있습니다. 즉, 표본의 대표성을 확보할 수 있는 방법입니다.\n현재 df_all 데이터를 모집단 데이터로 간주하고, 표본 데이터를 뽑고자 합니다. 이때 층화추출법을 사용하기 위해선 우선적으로 각 층(bsym)별 표본 크기가 주어져야 합니다. 이 과정을 표본 배분(sample allocation)이라고 합니다. 여기에서는 모집단 df_all의 bsym별 층 크기를 유지하기 위해 비례 배분(proportional allocation)을 사용하겠습니다. 비례 배분에 따른 표본 데이터의 표본 층 크기는 아래와 같습니다.\n\\[\nn_h = n\\times\\frac{N_h}{\\sum_{l=1}^HN_l}\n\\]\ndf_all 데이터의 30%만 샘플링한다고 하겠습니다. 그럴 경우 필요한 층(bsym)별 표본 크기(nh)는 아래와 같습니다.\n\n# 전체 데이터의 30%: 7495개\nround(nrow(df_all) * 0.3)\n\n[1] 7495\n\n# 모집단 층 크기: Nh, 표본 층 크기: nh\ndf_target &lt;- df_target %&gt;% \n  mutate(N = nrow(df_all)) %&gt;% \n  mutate(n = round(nrow(df_all) * 0.3)) %&gt;% \n  relocate(count_y, .after = n) %&gt;% \n  rename(Nh = count_y) %&gt;% \n  mutate(nh = round(n*Nh/N)) %&gt;% \n  group_by(bsym)\n\ndf_target %&gt;% print(n=25)\n\n# A tibble: 25 × 7\n# Groups:   bsym [25]\n   bsym    total_y    ratio     N     n    Nh    nh\n   &lt;chr&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;\n 1 2009-12     337 0.352880 24983  7495   955   287\n 2 2010-01     262 0.363889 24983  7495   720   216\n 3 2010-02     314 0.405685 24983  7495   774   232\n 4 2010-03     378 0.357616 24983  7495  1057   317\n 5 2010-04     345 0.366242 24983  7495   942   283\n 6 2010-05     368 0.380952 24983  7495   966   290\n 7 2010-06     392 0.376561 24983  7495  1041   312\n 8 2010-07     351 0.378233 24983  7495   928   278\n 9 2010-08     365 0.400659 24983  7495   911   273\n10 2010-09     463 0.404367 24983  7495  1145   344\n11 2010-10     657 0.438878 24983  7495  1497   449\n12 2010-11     524 0.326073 24983  7495  1607   482\n13 2010-12     324 0.366102 24983  7495   885   266\n14 2011-01     262 0.353576 24983  7495   741   222\n15 2011-02     290 0.382586 24983  7495   758   227\n16 2011-03     304 0.312115 24983  7495   974   292\n17 2011-04     368 0.429907 24983  7495   856   257\n18 2011-05     410 0.388258 24983  7495  1056   317\n19 2011-06     365 0.368315 24983  7495   991   297\n20 2011-07     388 0.408851 24983  7495   949   285\n21 2011-08     425 0.454545 24983  7495   935   281\n22 2011-09     489 0.386256 24983  7495  1266   380\n23 2011-10     622 0.456012 24983  7495  1364   409\n24 2011-11     371 0.222823 24983  7495  1665   500\n25 total      9374 1        24983  7495 24983  7495\n\n# 반올림 때문에 표본 층 크기가 n=7495 보다 1개 더 크므로 \n# 2011년 11월 표본 크기를 한 개 줄여주자\ndf_target$nh[df_target$bsym=='2011-11'] &lt;- df_target$nh[df_target$bsym=='2011-11'] - 1\n\n이제 주어진 표본배분안에 따라 bsym별 30% 정도의 표본을 뽑아보겠습니다.\n\nset.seed(123)\nord &lt;- unique(df_target$bsym)\nunits &lt;- sampling::strata(df_all, stratanames = \"bsym\", size = df_target$nh[1:24], method=\"srswor\")\n\ndf_all_sample &lt;- df_all %&gt;% \n  slice(units$ID_unit)\ndf_all_sample %&gt;% dim()\n\n[1] 7495    3\n\n\n이제 층화추출 후 표본 데이터 df_all_sample이 기존 모집단 데이터인 df_all의 bsym 별 target의 비율을 유지하는지 확인해 보겠습니다.\n\nCodelibrary(gt)\nlibrary(gtExtras)\ndf_tmp &lt;- df_all_sample %&gt;% \n  group_by(bsym) %&gt;% \n  reframe(sum_y = sum(target),\n          nh = n())\n   \ndf_tmp2 &lt;- df_tmp %&gt;% \n  mutate(target_ratio = sum_y/nh,\n         target_ratio_pop = df_target$ratio[1:24]) %&gt;% \n  pivot_longer(cols=c(target_ratio, target_ratio_pop), names_to = 'name') %&gt;% \n  group_by(bsym) %&gt;% \n  reframe(target_ratio = list(value))\ndf_tmp %&gt;% left_join(df_tmp2, by='bsym') %&gt;% \n  gt() %&gt;% \n  gt_theme_nytimes() %&gt;% \n  tab_header(title = \"bsym별 target 비율\") %&gt;% \n  gt_plt_bar_stack(column = target_ratio, labels = c(\"target_ratio\", \"target_ratio_pop\"), palette = c(\"skyblue\", \"hotpink\"))\n\n\n\n\n\n\nTable 1:  연도 및 월별 재구매 비율: 표본 vs 모집단 \n  \n\nbsym별 target 비율\n    \n\nbsym\n      sum_y\n      nh\n      \ntarget_ratio||target_ratio_pop\n\n    \n\n\n\n2009-12\n111\n287\n\n0.3870.353\n\n\n\n2010-01\n79\n216\n\n0.36570.3639\n\n\n\n2010-02\n78\n232\n\n0.3360.406\n\n\n\n2010-03\n116\n317\n\n0.36590.3576\n\n\n\n2010-04\n103\n283\n\n0.36400.3662\n\n\n\n2010-05\n107\n290\n\n0.3690.381\n\n\n\n2010-06\n131\n312\n\n0.4200.377\n\n\n\n2010-07\n102\n278\n\n0.3670.378\n\n\n\n2010-08\n108\n273\n\n0.39560.4007\n\n\n\n2010-09\n141\n344\n\n0.40990.4044\n\n\n\n2010-10\n189\n449\n\n0.4210.439\n\n\n\n2010-11\n166\n482\n\n0.3440.326\n\n\n\n2010-12\n120\n266\n\n0.4510.366\n\n\n\n2011-01\n77\n222\n\n0.34680.3536\n\n\n\n2011-02\n84\n227\n\n0.3700.383\n\n\n\n2011-03\n91\n292\n\n0.311640.31211\n\n\n\n2011-04\n123\n257\n\n0.4790.430\n\n\n\n2011-05\n121\n317\n\n0.38170.3883\n\n\n\n2011-06\n110\n297\n\n0.37040.3683\n\n\n\n2011-07\n114\n285\n\n0.40000.4089\n\n\n\n2011-08\n136\n281\n\n0.4840.455\n\n\n\n2011-09\n138\n380\n\n0.3630.386\n\n\n\n2011-10\n196\n409\n\n0.4790.456\n\n\n\n2011-11\n117\n499\n\n0.2340.223\n\n\n\n\n\n\n\n\n\n# 표본 전체 target ratio: 38.1%\n# 모집단 전체 target ratio: 37.5%\ndf_all_sample$target %&gt;% mean()\n\n[1] 0.3813209\n\ndf_all$target %&gt;% mean()\n\n[1] 0.3752151\n\n\n이렇게 구성한 df_all_sample 데이터를 가지고 Data Mart를 만드는 과정을 다음 포스팅에서 다루겠습니다."
  },
  {
    "objectID": "blog/2023-12-08-ecommerce-data-analysis(2)/index.html",
    "href": "blog/2023-12-08-ecommerce-data-analysis(2)/index.html",
    "title": "E-commerce 데이터 분석 (2)",
    "section": "",
    "text": "본 포스팅은 패스트캠퍼스 50개 프로젝트로 완벽하게 끝내는 머신러닝 시그니쳐의 강의내용을 바탕으로 참고하여 작성하였습니다.\n앞서 E-commerce 데이터 분석 (1)에서 A사가 운영하는 이커머스 플랫폼의 ②유입 고객 재구매를 촉진시키기 위해 7단계의 문제해결 프로세스를 정의했으며, Step 5의 데이터 분석 단계에 필요한 과정을 진행했습니다."
  },
  {
    "objectID": "blog/2023-12-08-ecommerce-data-analysis(2)/index.html#data-mart-기획-및-설계",
    "href": "blog/2023-12-08-ecommerce-data-analysis(2)/index.html#data-mart-기획-및-설계",
    "title": "E-commerce 데이터 분석 (2)",
    "section": "03-01. Data Mart 기획 및 설계",
    "text": "03-01. Data Mart 기획 및 설계\n\n\n모델링을 위한 Data Mart 기획\n\n\n\n\n고객 24,983명에 대한 거래 이력이 총 78만 건 정도였고, 이를 고객 데이터셋을 Undersampling 하여 추출한 분석 대상이 7,495명의 고객 데이터인 상황입니다. UCI Machine Learning Repository의 Online Retail 데이터를 Data Warehouse에서 가져온 데이터이고, 이를 이용해 고객별 구매 이력에 대한 Data Mart를 구성하려고 합니다. 이제 여러 가설을 세워 재구매 여부 target에 영향을 미치는 여러 변수(features)를 만들어 보겠습니다. 아래는 Data Mart를 구성하는 여러 feature 에 대한 설명과 로직이 작성되어 있는 Data Mart 기획서입니다.\n\n\nFigure 1: Data Mart 기획서"
  },
  {
    "objectID": "blog/2023-12-08-ecommerce-data-analysis(2)/index.html#data-추출-및-mart-개발",
    "href": "blog/2023-12-08-ecommerce-data-analysis(2)/index.html#data-추출-및-mart-개발",
    "title": "E-commerce 데이터 분석 (2)",
    "section": "03-02. Data 추출 및 Mart 개발",
    "text": "03-02. Data 추출 및 Mart 개발\n기본적인 전처리가 끝난 후 데이터를 df_origin로 저장되어 있습니다. 샘플링한 표본 데이터를 대상으로 Mart를 구성하기 위해서, 우선 df_origin과 표본 데이터 df_all_sample에 bsym과 CustomerID를 조합해 key 변수를 만들어 보겠습니다.\n\ndf_origin %&gt;% head()\n\n# A tibble: 6 × 9\n  InvoiceNo StockCode Description                           Quantity InvoiceDate         UnitPrice CustomerID Country        bsym   \n  &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;                                    &lt;int&gt; &lt;dttm&gt;                  &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;          &lt;chr&gt;  \n1 489434    85048     \"15CM CHRISTMAS GLASS BALL 20 LIGHTS\"       12 2009-12-01 07:45:00      6.95 13085      United Kingdom 2009-12\n2 489434    79323P    \"PINK CHERRY LIGHTS\"                        12 2009-12-01 07:45:00      6.75 13085      United Kingdom 2009-12\n3 489434    79323W    \"WHITE CHERRY LIGHTS\"                       12 2009-12-01 07:45:00      6.75 13085      United Kingdom 2009-12\n4 489434    22041     \"RECORD FRAME 7\\\" SINGLE SIZE\"              48 2009-12-01 07:45:00      2.1  13085      United Kingdom 2009-12\n5 489434    21232     \"STRAWBERRY CERAMIC TRINKET BOX\"            24 2009-12-01 07:45:00      1.25 13085      United Kingdom 2009-12\n6 489434    22064     \"PINK DOUGHNUT TRINKET POT\"                 24 2009-12-01 07:45:00      1.65 13085      United Kingdom 2009-12\n\ndf_all_sample %&gt;% head()\n\n# A tibble: 6 × 3\n  bsym    CustomerID target\n  &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt;\n1 2009-12 12682           1\n2 2009-12 15413           1\n3 2009-12 16321           0\n4 2009-12 15712           1\n5 2009-12 17700           1\n6 2009-12 14911           1\n\n# df_origin에 key 변수 생성\ndf_origin &lt;- df_origin %&gt;% \n  mutate(key = str_c(bsym, CustomerID))\ndf_origin %&gt;% \n  reframe(n_key = n_distinct(key))\n\n# A tibble: 1 × 1\n  n_key\n  &lt;int&gt;\n1 25598\n\n# df_all_sample에 key 변수 생성\ndf_all_sample &lt;- df_all_sample %&gt;% \n  mutate(key = str_c(bsym, CustomerID))\ndf_all_sample %&gt;% \n  reframe(n_key = n_distinct(key))\n\n# A tibble: 1 × 1\n  n_key\n  &lt;int&gt;\n1  7495\n\n\n이제 df_origin의 key를 이용해 df_all_sample에 존재하는 행들만 가져와 df_origin_sample 이라는 데이터를 만듭니다.\n\ndf_origin_sample &lt;- df_origin %&gt;% \n  filter(key %in% df_all_sample$key)\n\n# df_origin과 df_origin_sample의 비율: 대략 30% 정도\nnrow(df_origin_sample)/nrow(df_origin)\n\n[1] 0.2947758\n\n\nMart 구성\n구매금액\n\nIdea: 월별 구매금액에 따라 다음 달 재구매 확률이 다를 것이다\n\nMart 기획서의 첫 번째 구매금액 관련 3개의 변수를 만들기 위해 StockCode 당 구매금액을 나타내는 변수 amt를 UnitPrice * Quantity로 정의하겠습니다.\n\n# 1. 구매금액 amt 관련 변수 , max_amt, min_amt\n## 1) total_amt: 당월 총 구매금액\ndf_mart &lt;- df_origin_sample %&gt;%\n  mutate(amt = UnitPrice * Quantity) %&gt;% \n  group_by(bsym, CustomerID) %&gt;% \n  reframe(total_amt = sum(amt, na.rm = T))\n\n## 2) max_amt, min_amt: 당월 송장당 최대/최소 구매금액\ndf_mart &lt;- df_mart %&gt;% left_join(\n  df_origin_sample %&gt;% \n  mutate(amt = UnitPrice * Quantity) %&gt;% \n  group_by(bsym, CustomerID, InvoiceNo) %&gt;% \n  reframe(amt = sum(amt, na.rm = T)) %&gt;% \n  group_by(bsym, CustomerID) %&gt;% \n  reframe(max_amt = max(amt, na.rm = T),\n          min_amt = min(amt, na.rm = T)),\n  by = c(\"bsym\", \"CustomerID\")\n)\n\ndf_mart %&gt;% head()\n\n# A tibble: 6 × 5\n  bsym    CustomerID total_amt max_amt min_amt\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 2009-12 12437          578.    578.    578. \n2 2009-12 12523           74.6    74.6    74.6\n3 2009-12 12539         5149.   2583.   2566. \n4 2009-12 12557         1953.   1953.   1953. \n5 2009-12 12664          549.    549.    549. \n6 2009-12 12681         1015.   1015.   1015. \n\n\n구매건수\n\nIdea: 월별 구매건수에 따라 다음 달 재구매 확률이 다를 것이다\n\n이제 구매건수(cnt)와 관련된 변수 3가지를 만들겠습니다. 월별 총 구매건수는 total_cnt로, 월별 송장(InvoiceNo)별 구매 품목 수의 최대/최소는 max_cnt, min_cnt로 정의하겠습니다.\n\n# 2. 구매건수 cnt 관련 변수\n## 1) total_cnt: 당월 총 구매건수\ndf_mart &lt;- df_mart %&gt;% left_join(\n  df_origin_sample %&gt;% \n  group_by(bsym, CustomerID) %&gt;% \n  reframe(total_cnt = n_distinct(InvoiceNo)),\n  by = c(\"bsym\", \"CustomerID\")\n)\n## 2) max_cnt, min_cnt: InvoiceNo별 최대/최소 구매 품목 수\ndf_mart &lt;- df_mart %&gt;% \n  left_join(\n    df_origin_sample %&gt;% \n      group_by(bsym, CustomerID, InvoiceNo) %&gt;% \n      reframe(cnt = n_distinct(StockCode)) %&gt;% \n      group_by(bsym, CustomerID) %&gt;% \n      reframe(max_cnt = max(cnt, na.rm = T),\n              min_cnt = min(cnt, na.rm = T)),\n    by = c(\"bsym\", \"CustomerID\")\n  )\n\n\ndf_mart %&gt;% head()\n\n# A tibble: 6 × 8\n  bsym    CustomerID total_amt max_amt min_amt total_cnt max_cnt min_cnt\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;     &lt;int&gt;   &lt;int&gt;   &lt;int&gt;\n1 2009-12 12437          578.    578.    578.          1      27      27\n2 2009-12 12523           74.6    74.6    74.6         1       4       4\n3 2009-12 12539         5149.   2583.   2566.          2     104     103\n4 2009-12 12557         1953.   1953.   1953.          1       3       3\n5 2009-12 12664          549.    549.    549.          1       4       4\n6 2009-12 12681         1015.   1015.   1015.          1      46      46\n\n\n구매수량\n\nIdea: 월별 구매수량에 따라 다음 달 재구매 확률이 다를 것이다\n\n\n# 3. 구매수량 qty 관련 변수\n## 1) total_qty: 당월 총 구매수량\ndf_mart &lt;- df_mart %&gt;% left_join(\n  df_origin_sample %&gt;% \n    group_by(bsym, CustomerID) %&gt;% \n    reframe(total_qty = sum(Quantity, na.rm = T),\n            max_qty = max(Quantity, na.rm = T),\n            min_qty = min(Quantity, na.rm = T)),\n  by = c(\"bsym\", \"CustomerID\")\n)\n\ndf_mart %&gt;% head()\n\n# A tibble: 6 × 11\n  bsym    CustomerID total_amt max_amt min_amt total_cnt max_cnt min_cnt total_qty max_qty min_qty\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;     &lt;int&gt;   &lt;int&gt;   &lt;int&gt;     &lt;int&gt;   &lt;int&gt;   &lt;int&gt;\n1 2009-12 12437          578.    578.    578.          1      27      27       263      24       3\n2 2009-12 12523           74.6    74.6    74.6         1       4       4        62      36       4\n3 2009-12 12539         5149.   2583.   2566.          2     104     103      2128      48       2\n4 2009-12 12557         1953.   1953.   1953.          1       3       3       576     216     144\n5 2009-12 12664          549.    549.    549.          1       4       4       134      72       2\n6 2009-12 12681         1015.   1015.   1015.          1      46      46       650      72       1\n\n\n국적\n\nIdea: 국적에 따라 재구매 확률이 다를 것이다\n\n\n# 4. 국적 변수 생성\ndf_mart &lt;- df_mart %&gt;% left_join(\n  df_origin_sample %&gt;% \n    group_by(bsym, CustomerID) %&gt;% \n    reframe(Country = first(Country)),\n  by = c(\"bsym\", \"CustomerID\")\n)\ndf_mart %&gt;% head()\n\n# A tibble: 6 × 12\n  bsym    CustomerID total_amt max_amt min_amt total_cnt max_cnt min_cnt total_qty max_qty min_qty Country\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;     &lt;int&gt;   &lt;int&gt;   &lt;int&gt;     &lt;int&gt;   &lt;int&gt;   &lt;int&gt; &lt;chr&gt;  \n1 2009-12 12437          578.    578.    578.          1      27      27       263      24       3 France \n2 2009-12 12523           74.6    74.6    74.6         1       4       4        62      36       4 France \n3 2009-12 12539         5149.   2583.   2566.          2     104     103      2128      48       2 Spain  \n4 2009-12 12557         1953.   1953.   1953.          1       3       3       576     216     144 Spain  \n5 2009-12 12664          549.    549.    549.          1       4       4       134      72       2 Finland\n6 2009-12 12681         1015.   1015.   1015.          1      46      46       650      72       1 France \n\n\n구매 시간대\n\nIdea: 구매 시간대(아침, 점심, 저녁, 밤)에 따라 재구매 확률이 다를 것이다\n\n월별 고객별 아침, 점심, 저녁, 밤에 따른 구매 빈도를 구한 후 가장 많은 시간대를 peak_time이라는 이름의 feature로 선택하겠습니다.\n\n# 5. 구매 시간대(아침, 점심, 저녁, 밤)\n## 아침: 6~12시, 점심: 12~18시, 저녁: 18~24시, 밤: 0~6시\n## 시간대별 구매 빈도 계산\ndf_mart &lt;- df_mart %&gt;% left_join(\n  df_origin_sample %&gt;% \n    mutate(hour = hour(InvoiceDate),\n           peak_time = case_when(\n             hour &gt;= 6  & hour &lt; 12 ~ \"Morning\",\n             hour &gt;= 12 & hour &lt; 18 ~ \"Afternoon\",\n             hour &gt;= 18 & hour &lt; 24 ~ \"Evening\",\n             TRUE ~ \"Night\"\n           )) %&gt;% \n    group_by(bsym, CustomerID, peak_time) %&gt;% \n    reframe(purchase_cnt = n()) %&gt;% \n    group_by(bsym, CustomerID) %&gt;% \n    slice_max(purchase_cnt, n = 1, with_ties = FALSE) %&gt;% \n    select(-purchase_cnt) %&gt;% \n    ungroup(),\n  by = c(\"bsym\", \"CustomerID\")\n)\ndf_mart %&gt;% head()\n\n# A tibble: 6 × 13\n  bsym    CustomerID total_amt max_amt min_amt total_cnt max_cnt min_cnt total_qty max_qty min_qty Country peak_time\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;     &lt;int&gt;   &lt;int&gt;   &lt;int&gt;     &lt;int&gt;   &lt;int&gt;   &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;    \n1 2009-12 12437          578.    578.    578.          1      27      27       263      24       3 France  Afternoon\n2 2009-12 12523           74.6    74.6    74.6         1       4       4        62      36       4 France  Afternoon\n3 2009-12 12539         5149.   2583.   2566.          2     104     103      2128      48       2 Spain   Afternoon\n4 2009-12 12557         1953.   1953.   1953.          1       3       3       576     216     144 Spain   Morning  \n5 2009-12 12664          549.    549.    549.          1       4       4       134      72       2 Finland Morning  \n6 2009-12 12681         1015.   1015.   1015.          1      46      46       650      72       1 France  Afternoon\n\n\n계절\n\nIdea: 계절에 따라 재구매 확률이 다를 것이다\n\n\n# 6. 계절 변수 추가\ndf_mart &lt;- df_mart %&gt;% left_join(\n  df_origin_sample %&gt;% \n    mutate(month = month(InvoiceDate),\n           season = case_when(\n             month %in% c(3,4,5) ~ \"Spring\",\n             month %in% c(6,7,8) ~ \"Summer\",\n             month %in% c(9,10,11) ~ \"Autumn\",\n             TRUE ~ \"Winter\"\n           )) %&gt;% \n    group_by(bsym, CustomerID) %&gt;% \n    reframe(season = first(season)),\n  by = c(\"bsym\", \"CustomerID\")\n)\ndf_mart %&gt;% head()\n\n# A tibble: 6 × 14\n  bsym    CustomerID total_amt max_amt min_amt total_cnt max_cnt min_cnt total_qty max_qty min_qty Country peak_time season\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;     &lt;int&gt;   &lt;int&gt;   &lt;int&gt;     &lt;int&gt;   &lt;int&gt;   &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt; \n1 2009-12 12437          578.    578.    578.          1      27      27       263      24       3 France  Afternoon Winter\n2 2009-12 12523           74.6    74.6    74.6         1       4       4        62      36       4 France  Afternoon Winter\n3 2009-12 12539         5149.   2583.   2566.          2     104     103      2128      48       2 Spain   Afternoon Winter\n4 2009-12 12557         1953.   1953.   1953.          1       3       3       576     216     144 Spain   Morning   Winter\n5 2009-12 12664          549.    549.    549.          1       4       4       134      72       2 Finland Morning   Winter\n6 2009-12 12681         1015.   1015.   1015.          1      46      46       650      72       1 France  Afternoon Winter\n\n\n구매 빈도\n\nIdea: 구매 빈도가 높은 고객은 재구매 확률이 높을 것이다\n\n\n# 7. 당월 구매 빈도\n## freq = count(InvoiceNo) / # num of days in month\ndf_mart &lt;- df_mart %&gt;% left_join(\n  df_origin_sample %&gt;% \n    group_by(bsym, CustomerID) %&gt;% \n    reframe(cnt = n_distinct(InvoiceNo)) %&gt;% \n    mutate(\n      tmp_date = as.Date(paste0(bsym, \"-01\")),\n      days = as.integer(day(floor_date(tmp_date + months(1), \"month\") - 1)),\n      freq = cnt / days\n    ) %&gt;% \n    select(-c(cnt, tmp_date, days)),\n  by = c(\"bsym\", \"CustomerID\")\n)\n\ndf_mart %&gt;% head()\n\n# A tibble: 6 × 15\n  bsym    CustomerID total_amt max_amt min_amt total_cnt max_cnt min_cnt total_qty max_qty min_qty Country peak_time season   freq\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;     &lt;int&gt;   &lt;int&gt;   &lt;int&gt;     &lt;int&gt;   &lt;int&gt;   &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;   &lt;dbl&gt;\n1 2009-12 12437          578.    578.    578.          1      27      27       263      24       3 France  Afternoon Winter 0.0323\n2 2009-12 12523           74.6    74.6    74.6         1       4       4        62      36       4 France  Afternoon Winter 0.0323\n3 2009-12 12539         5149.   2583.   2566.          2     104     103      2128      48       2 Spain   Afternoon Winter 0.0645\n4 2009-12 12557         1953.   1953.   1953.          1       3       3       576     216     144 Spain   Morning   Winter 0.0323\n5 2009-12 12664          549.    549.    549.          1       4       4       134      72       2 Finland Morning   Winter 0.0323\n6 2009-12 12681         1015.   1015.   1015.          1      46      46       650      72       1 France  Afternoon Winter 0.0323\n\n\n평균 구매금액\n\nIdea: 평균 구매금액에 따라 재구매 확률이 다를 것이다\n\n이 변수는 단순히 total_amt를 total_cnt로 나눈 값입니다.\n\n# 8. 평균 구매금액: avg_amt\n## 송장당 평균 구매금액\ndf_mart &lt;- df_mart %&gt;% \n  mutate(avg_amt = total_amt / total_cnt)\n\ndf_mart %&gt;% head()\n\n# A tibble: 6 × 16\n  bsym    CustomerID total_amt max_amt min_amt total_cnt max_cnt min_cnt total_qty max_qty min_qty Country peak_time season   freq avg_amt\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;     &lt;int&gt;   &lt;int&gt;   &lt;int&gt;     &lt;int&gt;   &lt;int&gt;   &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 2009-12 12437          578.    578.    578.          1      27      27       263      24       3 France  Afternoon Winter 0.0323   578. \n2 2009-12 12523           74.6    74.6    74.6         1       4       4        62      36       4 France  Afternoon Winter 0.0323    74.6\n3 2009-12 12539         5149.   2583.   2566.          2     104     103      2128      48       2 Spain   Afternoon Winter 0.0645  2575. \n4 2009-12 12557         1953.   1953.   1953.          1       3       3       576     216     144 Spain   Morning   Winter 0.0323  1953. \n5 2009-12 12664          549.    549.    549.          1       4       4       134      72       2 Finland Morning   Winter 0.0323   549. \n6 2009-12 12681         1015.   1015.   1015.          1      46      46       650      72       1 France  Afternoon Winter 0.0323  1015. \n\n\n이제 target 변수와 병합해 저장하겠습니다.\n\ndf_mart &lt;- df_mart %&gt;% left_join(\n  df_all_sample %&gt;% select(-key),\n  by = c(\"bsym\", \"CustomerID\")\n)\n\ndf_mart %&gt;% head()\n\n# A tibble: 6 × 17\n  bsym    CustomerID total_amt max_amt min_amt total_cnt max_cnt min_cnt total_qty max_qty min_qty Country peak_time season   freq avg_amt target\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;     &lt;int&gt;   &lt;int&gt;   &lt;int&gt;     &lt;int&gt;   &lt;int&gt;   &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1 2009-12 12437          578.    578.    578.          1      27      27       263      24       3 France  Afternoon Winter 0.0323   578.       1\n2 2009-12 12523           74.6    74.6    74.6         1       4       4        62      36       4 France  Afternoon Winter 0.0323    74.6      0\n3 2009-12 12539         5149.   2583.   2566.          2     104     103      2128      48       2 Spain   Afternoon Winter 0.0645  2575.       0\n4 2009-12 12557         1953.   1953.   1953.          1       3       3       576     216     144 Spain   Morning   Winter 0.0323  1953.       0\n5 2009-12 12664          549.    549.    549.          1       4       4       134      72       2 Finland Morning   Winter 0.0323   549.       0\n6 2009-12 12681         1015.   1015.   1015.          1      46      46       650      72       1 France  Afternoon Winter 0.0323  1015.       1\n\ndf_mart %&gt;% dim()\n\n[1] 7495   17"
  },
  {
    "objectID": "blog/2023-12-08-ecommerce-data-analysis(2)/index.html#numeric-feature-engineering",
    "href": "blog/2023-12-08-ecommerce-data-analysis(2)/index.html#numeric-feature-engineering",
    "title": "E-commerce 데이터 분석 (2)",
    "section": "03-03. Numeric Feature Engineering",
    "text": "03-03. Numeric Feature Engineering\nFeature Engineering\n\nFeature Enigneering이란?\n\nFeature들을 재구성하여 모델을 효과적으로 사용하기 쉽게 하는 작업을 말합니다. 즉, 모델링의 성능을 향상시키기 위해 feature를 생성, 선택, 가공하는 일련의 모든 활동을 의미합니다. Feature의 주요한 특징들을 잘 나타낼 수 있도록 변수의 변환, encoding 등의 작업들이 포함됩니다.\n때로는 새로운 feature를 생성하기도 하고, 변환(transformation)도 가능합니다. 또한 Feature selection(변수 선택)과 Feature extraction(차원 축소)를 이용하기도 합니다.\n현재 저희의 목적은 다음 달 재구매 여부 target을 잘 분류하는 모델을 구축하는 Classification 문제에 적합한 feature engineering 과정을 적용하겠습니다.\nInformation Value\n이진 분류 문제에서 Event(여기서 target=1인 경우)와 Non Event의 비율을 고려할 때, 각 비율의 불균형한 정도를 확연하게 보고 싶을 때 사용하는 방법입니다. WoE(Weight of Evidence) 값을 아래와 같이 정의해보겠습니다.\n\\[\n\\begin{aligned}\n\\text{WoE} &= \\text{log}\\frac{p}{1-p}\\\\\nwhere\\quad p &= Pr(Event)\n\\end{aligned}\n\\]\n즉, WoE는 Event 비율의 Odds 값에 로그를 취한 Logit 값입니다. 어떤 Feature의 각 구간(bin)에 대한 (범주형일 경우 각 level) 정보 가치 값은 \\[IV_i = (Event_i\\% - NonEvent_i\\%)\\times WoE_i\\]으로 표현할 수 있고, 해당 Feature의 정보가치는 이를 모두 더한 \\[IV = \\sum IV_i = \\sum\\{(Event_i\\% - NonEvent_i\\%)\\times WoE_i\\}\\] 으로 정의할 수 있습니다.\n이러한 feature 별 IV 값에 대한 통상적인 기준은 아래와 같습니다.\n\n\nIV 값에 따른 예측력\n\n자세한 내용은 Logistic 예측 모형에서의 변수 선택 방법 - Information Value을 참고하시면 됩니다.\n이제 앞서 구성한 Data Mart(df_mart)의 각 feature에 대해서 IV 값을 구해보겠습니다.\n\ndf_mart %&gt;% \n  reframe(across(everything(), typeof))\n\n# A tibble: 1 × 17\n  bsym      CustomerID total_amt max_amt min_amt total_cnt max_cnt min_cnt total_qty max_qty min_qty Country   peak_time season    freq   avg_amt target\n  &lt;chr&gt;     &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt; \n1 character character  double    double  double  integer   integer integer integer   integer integer character character character double double  double\n\n\n우선 수치형 변수들에 대해 각각 5% 단위로 binning을 해주고, 40%, 75% 백분위수를 기준으로 3개의 그룹으로 구간화를 시켜주겠습니다.\n\niv_calculate &lt;- function(data, var){\n  quant &lt;- quantile(data[[var]], probs = c(0.40, 0.75))\n  iv &lt;- data %&gt;% \n    mutate(grp = findInterval(.data[[var]], quant, rightmost.closed = T) + 1,\n           n_target = ifelse(target==1, 0, 1)) %&gt;% \n    group_by(grp) %&gt;% \n    reframe(\n      target = sum(target, na.rm = T),\n      n_target = sum(n_target, na.rm = T)\n    ) %&gt;% \n    mutate(\n      good_pct = target / sum(target),\n      bad_pct = n_target / sum(n_target),\n      t_ratio = target / (target + n_target),\n      VAR = var,\n      iv = (good_pct - bad_pct) * log(good_pct / bad_pct)\n    ) \n  return(iv)\n}\n\nnumeric_cols &lt;- df_mart %&gt;% select_if(is.numeric) %&gt;% select(-target) %&gt;% colnames()\n\niv_df &lt;- map_dfr(numeric_cols, ~ iv_calculate(df_mart, .x))\nprint(iv_df, n = nrow(iv_df))\n\n# A tibble: 30 × 8\n     grp target n_target good_pct bad_pct t_ratio VAR             iv\n   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt;\n 1     1    853     2145    0.298  0.463    0.285 total_amt 0.0719  \n 2     2    974     1649    0.341  0.356    0.371 total_amt 0.000631\n 3     3   1031      843    0.361  0.182    0.550 total_amt 0.123   \n 4     1    909     2089    0.318  0.451    0.303 max_amt   0.0461  \n 5     2   1015     1608    0.355  0.347    0.387 max_amt   0.000200\n 6     3    934      940    0.327  0.203    0.498 max_amt   0.0593  \n 7     1   1113     1885    0.389  0.407    0.371 min_amt   0.000733\n 8     2   1015     1608    0.355  0.347    0.387 min_amt   0.000200\n 9     3    730     1144    0.255  0.247    0.390 min_amt   0.000302\n10     2   2350     4476    0.822  0.965    0.344 total_cnt 0.0229  \n11     3    508      161    0.178  0.0347   0.759 total_cnt 0.234   \n12     1   1037     1897    0.363  0.409    0.353 max_cnt   0.00555 \n13     2   1059     1675    0.371  0.361    0.387 max_cnt   0.000237\n14     3    762     1065    0.267  0.230    0.417 max_cnt   0.00551 \n15     1   1199     1716    0.420  0.370    0.411 min_cnt   0.00620 \n16     2   1004     1719    0.351  0.371    0.369 min_cnt   0.00104 \n17     3    655     1202    0.229  0.259    0.353 min_cnt   0.00370 \n18     1    886     2104    0.310  0.454    0.296 total_qty 0.0548  \n19     2    985     1647    0.345  0.355    0.374 total_qty 0.000318\n20     3    987      886    0.345  0.191    0.527 total_qty 0.0913  \n21     1    667     1435    0.233  0.309    0.317 max_qty   0.0215  \n22     2   1348     2259    0.472  0.487    0.374 max_qty   0.000502\n23     3    843      943    0.295  0.203    0.472 max_qty   0.0341  \n24     2   2128     3524    0.745  0.760    0.377 min_qty   0.000315\n25     3    730     1113    0.255  0.240    0.396 min_qty   0.000957\n26     2   2081     4195    0.728  0.905    0.332 freq      0.0383  \n27     3    777      442    0.272  0.0953   0.637 freq      0.185   \n28     1    963     2035    0.337  0.439    0.321 avg_amt   0.0269  \n29     2   1030     1593    0.360  0.344    0.393 avg_amt   0.000807\n30     3    865     1009    0.303  0.218    0.462 avg_amt   0.0281  \n\n# Feature 별 IV 값 합산\niv_df %&gt;% \n  group_by(VAR) %&gt;% \n  reframe(iv = sum(iv)) %&gt;% \n  arrange(desc(iv))\n\n# A tibble: 11 × 2\n   VAR            iv\n   &lt;chr&gt;       &lt;dbl&gt;\n 1 total_cnt 0.257  \n 2 freq      0.223  \n 3 total_amt 0.195  \n 4 total_qty 0.146  \n 5 max_amt   0.106  \n 6 max_qty   0.0560 \n 7 avg_amt   0.0558 \n 8 max_cnt   0.0113 \n 9 min_cnt   0.0109 \n10 min_qty   0.00127\n11 min_amt   0.00123\n\n\n수치형 변수들에 대해서 이렇게 직접 구간(bin)의 경계값을 지정해서 binning을 할 수 있습니다. 이번에는 target 변수를 잘 예측하는, 즉 변수별 IV 값이 높게 나오도록 구간화를 하는 방법을 소개하겠습니다. 이러한 방법을 optimized binning이라고 합니다.\ndlookr 패키지의 binning_by() 함수는 반응변수(target)을 가장 잘 예측하는 경계값으로 수치형 변수를 binning 하기 때문에 변수별로 bin의 개수가 다를 수 있으며, 수치형 변수가 target과 유의한 관계가 없는 경우 구간화를 하지 않아서 변수 선택을 고려할 수도 있습니다.\n\nlibrary(dlookr)\nnumeric_cols &lt;- df_mart %&gt;% select_if(is.numeric) %&gt;% select(-target) %&gt;% colnames()\n\n# binning_by() 이용 시 target과 유의하지 않은 변수는 구간화를 하지 않음\nbin_process &lt;- function(data, var){\n  tryCatch({\n    bin &lt;- binning_by(data, target, var)\n    attr(bin, \"name\") &lt;- var\n    return(bin)\n  }, warning = function(w){\n    bin &lt;- \"No significant splits\"\n    attr(bin, \"name\") &lt;- var\n    return(bin)\n  }, error = function(e){\n    bin &lt;- \"Error\"\n    attr(bin, \"name\") &lt;- var\n    return(bin)\n  })\n}\n\nbin_list &lt;- map(numeric_cols, ~bin_process(df_mart, .x))\n\n\n\n\n\n# target과 유의하지 않은 변수 확인:\nbin_list %&gt;% \n  keep(~typeof(.x) == \"character\") %&gt;% \n  map(~attr(.x, \"name\")) %&gt;% \n  unlist()\n\n[1] \"min_amt\" \"min_qty\"\n\n\n앞서 40%, 75% 백분위수 값을 경계로 IV 값을 구했 때 가장 낮은 IV 값을 가졌던 min_amt와 min_qty는 최적 구간으로 IV 값을 구했을 경우에도 target과 유의하지 않다고 판단되므로 고려하지 않겠습니다.\nOptimized binning이 각 수치형 변수의 몇 % 백분위수를 bin의 경계값으로 삼는지 확인해 보겠습니다.\n\nbin_cutoff &lt;- function(data, bin){\n  # cutoff level \n  cutoff &lt;- attr(bin, \"breaks\")\n  grp &lt;- c()\n  for(i in 1:(length(cutoff)-1)){\n    if(i==1){\n      brk &lt;- paste0(\"[\", cutoff[i], \",\", cutoff[i+1], \"]\")\n    } else{\n      brk &lt;- paste0(\"(\", cutoff[i], \",\", cutoff[i+1], \"]\")\n    }\n    grp &lt;- c(grp, brk)\n  }\n  attr(bin, \"levels\") &lt;- grp\n  # data의 수치형변수에서 해당 cutoff가 몇% 백분위수 인지\n  value &lt;- cutoff[2:(length(cutoff)-1)]\n  ecdf_func &lt;- ecdf(data[[attr(bin, \"name\")]])\n  percentile &lt;- ecdf_func(value)\n  return(data.frame(Var = attr(bin, \"name\"), \n                    cutoff = value,\n                    percentile = scales::percent(percentile, accuracy = 2)))\n}\n\nsig_bin_list &lt;- bin_list %&gt;% \n  keep(~typeof(.x) == \"integer\")\n\n# Optimized bin의 경계값 분위수 확인\nsig_bin_list %&gt;% \n  map(~bin_cutoff(df_mart, .x)) \n\n[[1]]\n        Var  cutoff percentile\n1 total_amt  138.70        14%\n2 total_amt  219.82        26%\n3 total_amt  542.10        70%\n4 total_amt 1878.72        94%\n\n[[2]]\n      Var  cutoff percentile\n1 max_amt  173.36        20%\n2 max_amt  477.46        72%\n3 max_amt 1136.68        94%\n\n[[3]]\n        Var cutoff percentile\n1 total_cnt      1        74%\n2 total_cnt      2        92%\n\n[[4]]\n      Var cutoff percentile\n1 max_cnt      3         8%\n2 max_cnt     44        86%\n\n[[5]]\n      Var cutoff percentile\n1 min_cnt      4        20%\n2 min_cnt     32        84%\n\n[[6]]\n        Var cutoff percentile\n1 total_qty     95        22%\n2 total_qty    235        58%\n3 total_qty    363        74%\n4 total_qty   1047        94%\n\n[[7]]\n      Var cutoff percentile\n1 max_qty     19        26%\n2 max_qty     90        84%\n3 max_qty    144        94%\n\n[[8]]\n   Var     cutoff percentile\n1 freq 0.03225806        42%\n2 freq 0.03571429        74%\n3 freq 0.07142857        92%\n\n[[9]]\n      Var    cutoff percentile\n1 avg_amt  154.6950        18%\n2 avg_amt  273.6600        44%\n3 avg_amt  521.0000        80%\n4 avg_amt  609.6733        84%\n5 avg_amt 1090.7600        94%\n\n# Optimized binning 후 IV 값 확인\nsig_bin_list %&gt;% \n  map(~data.frame(name = attr(.x, \"name\"),\n                  IV = attr(.x, \"performance\") %&gt;% pull(IV) %&gt;% .[length(.)])) %&gt;% \n  list_rbind() %&gt;% \n  arrange(desc(IV))\n\n       name      IV\n1 total_cnt 0.36390\n2      freq 0.36390\n3 total_amt 0.28501\n4 total_qty 0.21204\n5   max_amt 0.12493\n6   max_qty 0.08274\n7   avg_amt 0.07401\n8   max_cnt 0.03034\n9   min_cnt 0.01942\n\n\n낮은 IV 값을 가졌던 min_qty와 min_amt를 제외하고 비교해보면, 직접 구간화를 했을 때보다 전반적으로 IV 값들이 높아졌습니다.\nBinning 후 각 구간의 분포와 target이 1일 때의 분포 그래프를 확인할 수도 있습니다. 아래는 total_cnt의 최적 구간화 이후 각 구간별 분포와, target이 1인 것의 분포를 나타냅니다.\n\nCodelibrary(patchwork)\ntotal_cnt_bin &lt;- sig_bin_list %&gt;% \n  keep(~attr(.x, \"name\") == \"total_cnt\") %&gt;% .[[1]] \np1 &lt;- total_cnt_bin %&gt;% plot(type = \"freq\") +\n  theme(legend.position = \"none\")\np2 &lt;- total_cnt_bin %&gt;% plot(type = \"posrate\") +\n  theme(legend.position = \"none\")\np1 + p2\n\n\n\nFigure 2: total_cnt binning 후 분포"
  },
  {
    "objectID": "blog/2023-12-08-ecommerce-data-analysis(2)/index.html#categorical-feature-engineering",
    "href": "blog/2023-12-08-ecommerce-data-analysis(2)/index.html#categorical-feature-engineering",
    "title": "E-commerce 데이터 분석 (2)",
    "section": "03-03. Categorical Feature Engineering",
    "text": "03-03. Categorical Feature Engineering\n범주형 변수인 Country, peak_time, season에 대해서도 IV 값을 구해서 target 변수와의 관계를 살펴보겠습니다.\n우선 국적 변수인 Country에 대해 살펴보겠습니다.\n\n# 국적 Country 변수 unique한 값 수\ndf_mart %&gt;% \n  reframe(n_uniq = n_distinct(Country))\n\n# A tibble: 1 × 1\n  n_uniq\n   &lt;int&gt;\n1     33\n\n\n\nCodedf_mart %&gt;% \n  ggplot(aes(x=Country, fill = factor(target))) +\n  geom_bar(position = \"dodge\") +\n  theme_minimal() +\n  coord_flip() +\n  labs(fill = \"Target\", x = \"Country\", y = \"Count\")\n\n\n\nFigure 3: 국적별 target 분포\n\n\n\nFigure 3 을 보면 Country 변수의 대부분이 영국(United Kingdom)임을 알 수 있습니다. 따라서 영국을 제외한 나머지 값들은 기타 국가로 변경하겠습니다.\n\ndf_mart %&gt;% \n  count(Country) %&gt;% \n  arrange(desc(n)) %&gt;% \n  mutate(ratio = n/sum(n))\n\n# A tibble: 33 × 3\n   Country            n   ratio\n   &lt;chr&gt;          &lt;int&gt;   &lt;dbl&gt;\n 1 United Kingdom  6871 0.917  \n 2 Germany          150 0.0200 \n 3 France           144 0.0192 \n 4 Belgium           38 0.00507\n 5 Spain             33 0.00440\n 6 Australia         25 0.00334\n 7 Italy             25 0.00334\n 8 Netherlands       23 0.00307\n 9 Switzerland       23 0.00307\n10 Portugal          22 0.00294\n# ℹ 23 more rows\n\n# UK 이외의 기타 국가로 처리\ndf_mart &lt;- df_mart %&gt;% \n  mutate(Country = ifelse(Country==\"United Kingdom\", \"UK\", \"ETC\")) \n\n이제 범주형 변수인 Country, peak_time, season의 IV 값을 구해보겠습니다.\n\nlibrary(scorecard)\nCountry_iv &lt;- woebin(df_mart, \"target\", \"Country\")\n\n✔ Binning on 7495 rows and 2 columns in 00:00:01\n\npeak_iv &lt;- woebin(df_mart, \"target\", \"peak_time\")\n\n✔ Binning on 7495 rows and 2 columns in 00:00:01\n\nseason_iv &lt;- woebin(df_mart, \"target\", \"season\")\n\n✔ Binning on 7495 rows and 2 columns in 00:00:01\n\n# Country IV \nCountry_iv$Country %&gt;% tibble()\n\n# A tibble: 2 × 12\n  variable bin   count count_distr   neg   pos posprob      woe  bin_iv total_iv\n  &lt;chr&gt;    &lt;chr&gt; &lt;int&gt;       &lt;dbl&gt; &lt;int&gt; &lt;int&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 Country  ETC     624      0.0833   387   237   0.380 -6.42e-3 3.43e-6  3.74e-6\n2 Country  UK     6871      0.917   4250  2621   0.381  5.82e-4 3.11e-7  3.74e-6\n# ℹ 2 more variables: breaks &lt;chr&gt;, is_special_values &lt;lgl&gt;\n\n# peak_time IV\npeak_iv$peak_time %&gt;% tibble()\n\n# A tibble: 2 × 12\n  variable  bin   count count_distr   neg   pos posprob     woe  bin_iv total_iv\n  &lt;chr&gt;     &lt;chr&gt; &lt;int&gt;       &lt;dbl&gt; &lt;int&gt; &lt;int&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 peak_time Afte…  4845       0.646  3019  1826   0.377 -0.0189 2.29e-4 0.000646\n2 peak_time Morn…  2650       0.354  1618  1032   0.389  0.0343 4.16e-4 0.000646\n# ℹ 2 more variables: breaks &lt;chr&gt;, is_special_values &lt;lgl&gt;\n\n# season IV\nseason_iv$season %&gt;% tibble()\n\n# A tibble: 3 × 12\n  variable bin    count count_distr   neg   pos posprob     woe  bin_iv total_iv\n  &lt;chr&gt;    &lt;chr&gt;  &lt;int&gt;       &lt;dbl&gt; &lt;int&gt; &lt;int&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 season   Autumn  2563       0.342  1616   947   0.369 -0.0505 8.65e-4  0.00350\n2 season   Sprin…  3206       0.428  1996  1210   0.377 -0.0166 1.17e-4  0.00350\n3 season   Summer  1726       0.230  1025   701   0.406  0.104  2.52e-3  0.00350\n# ℹ 2 more variables: breaks &lt;chr&gt;, is_special_values &lt;lgl&gt;\n\n\ntarget 변수에 대한 범주형 변수들의 IV 값을 보면 굉장히 낮으므로 이 세 변수는 target 변수와 유의미한 관계가 없어 보입니다."
  }
]